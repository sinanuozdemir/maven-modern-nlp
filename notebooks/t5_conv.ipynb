{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61b84c5a",
   "metadata": {},
   "source": [
    "# Our plan of action\n",
    "![](../images/Conversation%20Q_A%20Prototype.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaaccb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.philschmid.de/fine-tune-flan-t5\n",
    "from datasets import load_dataset, Dataset\n",
    "from random import randrange\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, \\\n",
    "    Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq, pipeline\n",
    "\n",
    "import os\n",
    "import openai\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59c98219",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset samsum (/Users/sinanozdemir/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00d706cb612a46ff81f808288e64d361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 14732\n",
      "Test dataset size: 819\n"
     ]
    }
   ],
   "source": [
    "dataset_id = \"samsum\"\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(dataset_id)\n",
    "\n",
    "print(f\"Train dataset size: {len(dataset['train'])}\")\n",
    "print(f\"Test dataset size: {len(dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e59371fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dialogue: \n",
      "Brooke: Hi ðŸ™‚\r\n",
      "Brooke: Just wanted to let you know that I just finished Coelho.\r\n",
      "Sheila: Hi there ðŸ™‚\r\n",
      "Sheila: Nice. Did you enjoy it?\r\n",
      "Brooke: I first read The Pilgrimage and had an idea on how he writes and thinks as an author.\r\n",
      "Brooke: But that book you gave me, Eleven Minutes, was so different!\r\n",
      "Sheila: He can write about anything, but one thing that stays the same is that it's always gonna be very emotional.\r\n",
      "Brooke: I cried while reading both of them. But they moved me in a different way.\r\n",
      "Sheila: I didn't read The Pilgrimage, the title alone suggests I wouldn't be interested in that story.\r\n",
      "Brooke: If you wish, I can always borrow you this one.\r\n",
      "Sheila: I'll keep that in mind.\r\n",
      "Sheila: I'm glad you enjoyed Eleven Minutes as much as I did.\r\n",
      "Brooke: Are you free tomorrow at 18:00?\r\n",
      "Sheila: I might be, why?\r\n",
      "Brooke: I will be in your neighbourhood and I can drop by to give it back.\r\n",
      "Sheila: You can come, but I gotta leave at 7.\r\n",
      "Brooke: Sure, I'll be on time!\r\n",
      "Sheila: See you tomorrow at 6 then.\r\n",
      "Brooke: See you!\r\n",
      "Brooke: And thanks once again ðŸ™‚\n",
      "---------------\n",
      "summary: \n",
      "Brooke has just finished reading a book Eleven Minutes written by Coelho. The Pilgrim was the first book of this author she has read. Brooke cried while reading both of these books. Brooke and Sheila will meet tomorrow at 6 p.m. Brooke will return Sheila her book. \n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "sample = dataset['train'][randrange(len(dataset[\"train\"]))]\n",
    "print(f\"dialogue: \\n{sample['dialogue']}\\n---------------\")\n",
    "print(f\"summary: \\n{sample['summary']}\\n---------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9be30367",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id=\"google/flan-t5-small\"\n",
    "\n",
    "# Load tokenizer of FLAN-T5\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a3792c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/sinanozdemir/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-6fd195173404bd74.arrow\n",
      "Loading cached processed dataset at /Users/sinanozdemir/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-2a5c805eca4875ec.arrow\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(sample):\n",
    "    # add prefix to the input for t5\n",
    "    inputs = [\"summarize: \" + item for item in sample[\"dialogue\"]]\n",
    "\n",
    "    # tokenize inputs\n",
    "    model_inputs = tokenizer(inputs, truncation=True)\n",
    "\n",
    "    # Tokenize targets with the `text_target` keyword argument\n",
    "    labels = tokenizer(text_target=sample[\"summary\"], truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "dataset['tokenized_train'] = dataset['train'].map(preprocess_function, batched=True, remove_columns=[\"dialogue\", \"summary\", \"id\"])\n",
    "dataset['tokenized_test'] = dataset['test'].map(preprocess_function, batched=True, remove_columns=[\"dialogue\", \"summary\", \"id\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f997d91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: \n",
      "[21603, 10, 29298, 10, 1521, 15480, 11, 239, 234, 58, 10445, 10, 150, 855, 29298, 10, 2049, 10445, 10, 18364, 541, 58, 1]\n",
      "---------------\n",
      "labels: \n",
      "[29298, 31, 7, 1362, 33, 29, 31, 17, 234, 5, 3, 1]\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "sample = dataset['tokenized_train'][randrange(len(dataset[\"tokenized_train\"]))]\n",
    "print(f\"input_ids: \\n{sample['input_ids']}\\n---------------\")\n",
    "print(f\"labels: \\n{sample['labels']}\\n---------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "918219fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# huggingface hub model id\n",
    "model_id=\"google/flan-t5-small\"\n",
    "\n",
    "# load model from the hub\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f427712f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# we want to ignore tokenizer pad token in the loss\n",
    "# label_pad_token_id = -100\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=tokenizer.pad_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5127f2b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8524ae56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Hugging Face repository id\n",
    "repository_id = f\"{model_id.split('/')[1]}-{dataset_id}\"\n",
    "\n",
    "# Define training args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=repository_id,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    predict_with_generate=False,\n",
    "    fp16=False, # Overflows with fp16\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=1,\n",
    "    # logging & evaluation strategies\n",
    "    logging_dir=f\"{repository_id}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"tokenized_train\"],\n",
    "    eval_dataset=dataset[\"tokenized_test\"],\n",
    "    data_collator=data_collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8265944",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 819\n",
      "  Batch size = 32\n",
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='26' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [26/26 00:50]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 31.901212692260742,\n",
       " 'eval_runtime': 53.1241,\n",
       " 'eval_samples_per_second': 15.417,\n",
       " 'eval_steps_per_second': 0.489}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ebd250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train()  # not running because someone else already did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8597f927",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c036d249",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bc4d428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dialogue: \n",
      "Richie: Pogba\r\n",
      "Clay: Pogboom\r\n",
      "Richie: what a s strike yoh!\r\n",
      "Clay: was off the seat the moment he chopped the ball back to his right foot\r\n",
      "Richie: me too dude\r\n",
      "Clay: hope his form lasts\r\n",
      "Richie: This season he's more mature\r\n",
      "Clay: Yeah, Jose has his trust in him\r\n",
      "Richie: everyone does\r\n",
      "Clay: yeah, he really deserved to score after his first 60 minutes\r\n",
      "Richie: reward\r\n",
      "Clay: yeah man\r\n",
      "Richie: cool then \r\n",
      "Clay: cool\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "# select a random test sample\n",
    "sample = dataset['test'][randrange(len(dataset[\"test\"]))]\n",
    "print(f\"dialogue: \\n{sample['dialogue']}\\n---------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60c77cc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/sinanozdemir/.cache/huggingface/hub/models--philschmid--flan-t5-base-samsum/snapshots/dab5b3234360d786065ab29c6dd5af00a0bff05f/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"philschmid/flan-t5-base-samsum\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /Users/sinanozdemir/.cache/huggingface/hub/models--philschmid--flan-t5-base-samsum/snapshots/dab5b3234360d786065ab29c6dd5af00a0bff05f/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at philschmid/flan-t5-base-samsum.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# base_model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"philschmid/flan-t5-base-samsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e5733de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base flan-t5 summary:\n",
      "[\"Mickey, Jessica and Ollie are talking about a sculpture garden in Finnland. It's a sculpture garden in Finnland.\"]\n"
     ]
    }
   ],
   "source": [
    "# summarize dialogue\n",
    "summary = tokenizer.batch_decode(\n",
    "    base_model.generate(**tokenizer.encode_plus(f'summarize: {sample[\"dialogue\"]}', \n",
    "                                                     return_tensors='pt'), \n",
    "                             num_beams=4,\n",
    "                             min_length=30, max_length=200\n",
    "                            ), skip_special_tokens=True\n",
    ")\n",
    "print(f\"base flan-t5 summary:\\n{summary}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cab33177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fine-tuned flan-t5 summary:\n",
      "[\"Kelly's place is in a sculpture garden in Finnland. There are a hundred of human-sized figures in the garden. Ollie and Mickey are going to Nagoro village in Japan.\"]\n"
     ]
    }
   ],
   "source": [
    "# summarize dialogue\n",
    "summary = tokenizer.batch_decode(\n",
    "    model.generate(**tokenizer.encode_plus(f'summarize: {sample[\"dialogue\"]}', \n",
    "                                                     return_tensors='pt'), \n",
    "                             num_beams=4,\n",
    "                             min_length=30, max_length=200\n",
    "                            ), skip_special_tokens=True\n",
    ")\n",
    "print(f\"fine-tuned flan-t5 summary:\\n{summary}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22d560f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74be3c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model and tokenizer from huggingface hub with pipeline\n",
    "summarizer = pipeline(\"summarization\", model=\"philschmid/flan-t5-base-samsum\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ca8de377",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 200, but you input_length is only 131. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=65)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dialogue: \n",
      "Harry: heyyyy are you there??\n",
      "Cindy: Yes dear what is it?\n",
      "Harry: Can you call Ela and tell her i need to talk urgent please pick my call.\n",
      "Cindy: what happened now? an other fight :O\n",
      "Harry: please tell her\n",
      "Cindy: MAN! you guys... am i some kind of a messenger service here?\n",
      "Harry: PLEASEEEEEEEEE ?\n",
      "Cindy: ok doing.... but thats the last time.\n",
      "Harry: Yes like always:P\n",
      "Cindy: Hate you seriously man.\n",
      "Harry: Thank you\n",
      "Cindy: Done you can call her now.\n",
      "---------------\n",
      "flan-t5 summary:\n",
      "Harry wants Cindy to call Ela and tell her he needs to talk urgently. Cindy is a messenger service at Harry's place.\n"
     ]
    }
   ],
   "source": [
    "# select a random test sample\n",
    "sample = dataset['test'][randrange(len(dataset[\"test\"]))]\n",
    "print(f\"dialogue: \\n{sample['dialogue']}\\n---------------\")\n",
    "\n",
    "# summarize dialogue\n",
    "res = summarizer(sample[\"dialogue\"])\n",
    "\n",
    "print(f\"flan-t5 summary:\\n{res[0]['summary_text']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7d4abff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Joy sends Peter a link to her beauty shop',\n",
       " 'Peter will look at it',\n",
       " 'Joy will let him know if he should ask any questions.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers = [_ for _ in res[0]['summary_text'].split('. ') if len(_) > 2]\n",
    "\n",
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "694bb77f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "def convert_a_to_q(answers, model='text-davinci-003'):\n",
    "    response = openai.Completion.create(\n",
    "      model=model,\n",
    "      prompt=f\"convert these answers into questions:\\n\" + '\\n'.join([f'{index+1}. {answer}' for index, answer in enumerate(answers)]),\n",
    "      temperature=0.2,\n",
    "      max_tokens=256,\n",
    "      top_p=0.1\n",
    "    )\n",
    "#     print(response.choices[0].text.strip())\n",
    "    return list(zip([x[3:] if '. ' in x else x for x in (response.choices[0].text.strip()).split('\\n')], answers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "989b22d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Justin has seen the last episode of Game of Thrones.',\n",
       "  'The weather in Poland is getting cooler'),\n",
       " (\"Justin can't wait to see it.\",\n",
       "  \"Justin hasn't seen the last episode of Game of Thrones yet\"),\n",
       " ('Justin has seen the last episode of Game of Thrones.',\n",
       "  \"Justin can't wait to see it.\")]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_a_to_q(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5422fa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1a707f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c222f694",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                 | 0/100 [00:00<?, ?it/s]Your max_length is set to 200, but you input_length is only 30. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=15)\n",
      "  1%|â–                                        | 1/100 [00:05<09:45,  5.92s/it]Your max_length is set to 200, but you input_length is only 29. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=14)\n",
      "  2%|â–Š                                        | 2/100 [00:10<08:09,  4.99s/it]Your max_length is set to 200, but you input_length is only 154. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=77)\n",
      "  3%|â–ˆâ–                                       | 3/100 [00:15<07:55,  4.90s/it]Your max_length is set to 200, but you input_length is only 57. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=28)\n",
      "  5%|â–ˆâ–ˆ                                       | 5/100 [00:24<07:50,  4.95s/it]Your max_length is set to 200, but you input_length is only 119. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=59)\n",
      "  6%|â–ˆâ–ˆâ–                                      | 6/100 [00:28<07:24,  4.73s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (569 > 512). Running this sequence through the model will result in indexing errors\n",
      "  8%|â–ˆâ–ˆâ–ˆâ–Ž                                     | 8/100 [00:43<09:02,  5.90s/it]Your max_length is set to 200, but you input_length is only 82. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=41)\n",
      " 10%|â–ˆâ–ˆâ–ˆâ–ˆ                                    | 10/100 [00:52<07:48,  5.20s/it]Your max_length is set to 200, but you input_length is only 110. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=55)\n",
      " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                   | 11/100 [01:00<08:44,  5.89s/it]Your max_length is set to 200, but you input_length is only 25. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=12)\n",
      " 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š                                   | 12/100 [01:04<08:04,  5.51s/it]Your max_length is set to 200, but you input_length is only 40. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=20)\n",
      " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                  | 13/100 [01:08<07:11,  4.96s/it]Your max_length is set to 200, but you input_length is only 172. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=86)\n",
      " 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                  | 14/100 [01:13<07:17,  5.08s/it]Your max_length is set to 200, but you input_length is only 172. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=86)\n",
      " 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                 | 16/100 [01:24<07:10,  5.12s/it]Your max_length is set to 200, but you input_length is only 77. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=38)\n",
      " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                 | 17/100 [01:28<06:36,  4.77s/it]Your max_length is set to 200, but you input_length is only 113. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=56)\n",
      " 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                | 18/100 [01:35<07:37,  5.58s/it]Your max_length is set to 200, but you input_length is only 71. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=35)\n",
      " 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                | 19/100 [01:41<07:33,  5.60s/it]Your max_length is set to 200, but you input_length is only 65. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=32)\n",
      " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                               | 22/100 [01:59<07:59,  6.15s/it]Your max_length is set to 200, but you input_length is only 25. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=12)\n",
      " 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                              | 24/100 [02:10<07:37,  6.02s/it]Your max_length is set to 200, but you input_length is only 69. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=34)\n",
      " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 25/100 [02:17<07:53,  6.31s/it]Your max_length is set to 200, but you input_length is only 125. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=62)\n",
      " 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                             | 26/100 [02:23<07:28,  6.06s/it]Your max_length is set to 200, but you input_length is only 91. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=45)\n",
      " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                             | 27/100 [02:27<06:45,  5.56s/it]Your max_length is set to 200, but you input_length is only 150. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=75)\n",
      " 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 28/100 [02:31<06:06,  5.09s/it]Your max_length is set to 200, but you input_length is only 169. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=84)\n",
      " 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                            | 29/100 [02:37<06:14,  5.27s/it]Your max_length is set to 200, but you input_length is only 154. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=77)\n",
      " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 31/100 [02:54<08:32,  7.43s/it]Your max_length is set to 200, but you input_length is only 146. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=73)\n",
      " 32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                           | 32/100 [03:01<08:12,  7.24s/it]Your max_length is set to 200, but you input_length is only 48. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=24)\n",
      " 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 34/100 [03:08<05:56,  5.40s/it]Your max_length is set to 200, but you input_length is only 156. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=78)\n",
      " 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                          | 35/100 [03:13<05:33,  5.12s/it]Your max_length is set to 200, but you input_length is only 197. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=98)\n",
      " 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 36/100 [03:17<05:09,  4.84s/it]Your max_length is set to 200, but you input_length is only 175. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=87)\n",
      " 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 38/100 [03:30<05:43,  5.54s/it]Your max_length is set to 200, but you input_length is only 173. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=86)\n",
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                       | 42/100 [04:00<07:00,  7.25s/it]Your max_length is set to 200, but you input_length is only 117. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=58)\n",
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 46/100 [04:24<05:44,  6.37s/it]Your max_length is set to 200, but you input_length is only 141. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=70)\n",
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 48/100 [04:33<04:28,  5.16s/it]Your max_length is set to 200, but you input_length is only 84. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=42)\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 50/100 [04:46<05:00,  6.02s/it]Your max_length is set to 200, but you input_length is only 43. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=21)\n",
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 51/100 [04:50<04:31,  5.53s/it]Your max_length is set to 200, but you input_length is only 153. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=76)\n",
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                   | 52/100 [04:54<04:08,  5.18s/it]Your max_length is set to 200, but you input_length is only 48. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=24)\n",
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 53/100 [05:00<04:08,  5.29s/it]Your max_length is set to 200, but you input_length is only 154. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=77)\n",
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                  | 54/100 [05:06<04:09,  5.42s/it]Your max_length is set to 200, but you input_length is only 194. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=97)\n",
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                  | 55/100 [05:09<03:42,  4.94s/it]Your max_length is set to 200, but you input_length is only 104. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=52)\n",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                 | 57/100 [05:25<04:36,  6.42s/it]Your max_length is set to 200, but you input_length is only 167. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=83)\n",
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 58/100 [05:29<04:08,  5.92s/it]Your max_length is set to 200, but you input_length is only 104. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=52)\n",
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                | 59/100 [05:34<03:51,  5.65s/it]Your max_length is set to 200, but you input_length is only 113. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=56)\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 60/100 [05:40<03:41,  5.53s/it]Your max_length is set to 200, but you input_length is only 193. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=96)\n",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š               | 62/100 [05:55<04:06,  6.50s/it]Your max_length is set to 200, but you input_length is only 67. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=33)\n",
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ              | 64/100 [06:06<03:31,  5.87s/it]Your max_length is set to 200, but you input_length is only 46. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=23)\n",
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ              | 65/100 [06:11<03:12,  5.50s/it]Your max_length is set to 200, but you input_length is only 67. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=33)\n",
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 66/100 [06:16<03:05,  5.46s/it]Your max_length is set to 200, but you input_length is only 117. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=58)\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š             | 67/100 [06:20<02:44,  4.98s/it]Your max_length is set to 200, but you input_length is only 123. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=61)\n",
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 68/100 [06:26<02:53,  5.43s/it]Your max_length is set to 200, but you input_length is only 165. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=82)\n",
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ            | 69/100 [06:32<02:50,  5.49s/it]Your max_length is set to 200, but you input_length is only 50. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=25)\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            | 70/100 [06:38<02:50,  5.69s/it]Your max_length is set to 200, but you input_length is only 28. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=14)\n",
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 71/100 [06:44<02:47,  5.77s/it]Your max_length is set to 200, but you input_length is only 68. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=34)\n",
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š           | 72/100 [06:50<02:44,  5.88s/it]Your max_length is set to 200, but you input_length is only 145. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=72)\n",
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 73/100 [06:55<02:26,  5.43s/it]Your max_length is set to 200, but you input_length is only 116. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=58)\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 75/100 [07:05<02:08,  5.13s/it]Your max_length is set to 200, but you input_length is only 86. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=43)\n",
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š         | 77/100 [07:16<02:05,  5.46s/it]Your max_length is set to 200, but you input_length is only 80. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=40)\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 78/100 [07:22<02:05,  5.69s/it]Your max_length is set to 200, but you input_length is only 90. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=45)\n",
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 81/100 [07:42<02:00,  6.36s/it]Your max_length is set to 200, but you input_length is only 75. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=37)\n",
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š       | 82/100 [07:46<01:45,  5.86s/it]Your max_length is set to 200, but you input_length is only 50. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=25)\n",
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ      | 84/100 [07:58<01:35,  5.95s/it]Your max_length is set to 200, but you input_length is only 150. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=75)\n",
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      | 85/100 [08:02<01:20,  5.34s/it]Your max_length is set to 200, but you input_length is only 147. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=73)\n",
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 86/100 [08:06<01:09,  4.96s/it]Your max_length is set to 200, but you input_length is only 31. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=15)\n",
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 87/100 [08:10<01:01,  4.74s/it]Your max_length is set to 200, but you input_length is only 103. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=51)\n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 88/100 [08:16<01:00,  5.06s/it]Your max_length is set to 200, but you input_length is only 180. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=90)\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 90/100 [08:28<00:55,  5.52s/it]Your max_length is set to 200, but you input_length is only 38. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=19)\n",
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 91/100 [08:32<00:45,  5.05s/it]Your max_length is set to 200, but you input_length is only 35. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=17)\n",
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 92/100 [08:35<00:36,  4.56s/it]Your max_length is set to 200, but you input_length is only 95. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=47)\n",
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 93/100 [08:40<00:32,  4.58s/it]Your max_length is set to 200, but you input_length is only 98. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=49)\n",
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 94/100 [08:46<00:29,  4.99s/it]Your max_length is set to 200, but you input_length is only 77. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=38)\n",
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 95/100 [08:50<00:22,  4.60s/it]Your max_length is set to 200, but you input_length is only 47. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=23)\n",
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 97/100 [09:01<00:15,  5.31s/it]Your max_length is set to 200, but you input_length is only 122. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=61)\n",
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 98/100 [09:07<00:11,  5.63s/it]Your max_length is set to 200, but you input_length is only 84. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 99/100 [09:11<00:05,  5.06s/it]Your max_length is set to 200, but you input_length is only 62. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=31)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [09:17<00:00,  5.57s/it]\n"
     ]
    }
   ],
   "source": [
    "conv_data = [] \n",
    "NUM_TO_USE = 100\n",
    "\n",
    "for dialogue in tqdm(dataset['train'][:NUM_TO_USE]['dialogue']):\n",
    "    res = summarizer(dialogue)\n",
    "    answers = [_ for _ in res[0]['summary_text'].split('. ') if len(_) > 2]\n",
    "    random_dialogue = dataset['test'][randrange(len(dataset['test']))]['dialogue']\n",
    "    for question, answer in convert_a_to_q(answers):\n",
    "        if len(question) >= 20:\n",
    "            conv_data.append({'question': question, 'prompt': question + ': ' + dialogue, 'answer': answer})\n",
    "\n",
    "            # pair questions with irrelevant conv to generate: \"This transcript does not say.\"\n",
    "            conv_data.append({'question': question, 'prompt': question + ': ' + random_dialogue, 'answer': 'I cannot be sure given this transcript'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8b194f79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>prompt</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Did Amanda bake cookies for Jerry?</td>\n",
       "      <td>Did Amanda bake cookies for Jerry?: Amanda: I ...</td>\n",
       "      <td>Amanda baked cookies for Jerry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Did Amanda bake cookies for Jerry?</td>\n",
       "      <td>Did Amanda bake cookies for Jerry?: Val: it's ...</td>\n",
       "      <td>I cannot be sure given this transcript</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>When will she bring them?</td>\n",
       "      <td>When will she bring them?: Amanda: I baked  co...</td>\n",
       "      <td>She will bring them tomorrow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>When will she bring them?</td>\n",
       "      <td>When will she bring them?: Val: it's raining!\\...</td>\n",
       "      <td>I cannot be sure given this transcript</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Will Amanda bring Jerry some cookies?</td>\n",
       "      <td>Will Amanda bring Jerry some cookies?: Amanda:...</td>\n",
       "      <td>Amanda will bring Jerry some cookies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>Is the weather in Poland getting cooler?</td>\n",
       "      <td>Is the weather in Poland getting cooler?: Adel...</td>\n",
       "      <td>I cannot be sure given this transcript</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>Has Justin seen the last episode of Game of Th...</td>\n",
       "      <td>Has Justin seen the last episode of Game of Th...</td>\n",
       "      <td>Justin hasn't seen the last episode of Game of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>Has Justin seen the last episode of Game of Th...</td>\n",
       "      <td>Has Justin seen the last episode of Game of Th...</td>\n",
       "      <td>I cannot be sure given this transcript</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>Why can't Justin wait to see it?</td>\n",
       "      <td>Why can't Justin wait to see it?: Tom: Howâ€™s t...</td>\n",
       "      <td>Justin can't wait to see it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>Why can't Justin wait to see it?</td>\n",
       "      <td>Why can't Justin wait to see it?: Adele: i am ...</td>\n",
       "      <td>I cannot be sure given this transcript</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>562 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              question  \\\n",
       "0                   Did Amanda bake cookies for Jerry?   \n",
       "1                   Did Amanda bake cookies for Jerry?   \n",
       "2                            When will she bring them?   \n",
       "3                            When will she bring them?   \n",
       "4                Will Amanda bring Jerry some cookies?   \n",
       "..                                                 ...   \n",
       "557           Is the weather in Poland getting cooler?   \n",
       "558  Has Justin seen the last episode of Game of Th...   \n",
       "559  Has Justin seen the last episode of Game of Th...   \n",
       "560                   Why can't Justin wait to see it?   \n",
       "561                   Why can't Justin wait to see it?   \n",
       "\n",
       "                                                prompt  \\\n",
       "0    Did Amanda bake cookies for Jerry?: Amanda: I ...   \n",
       "1    Did Amanda bake cookies for Jerry?: Val: it's ...   \n",
       "2    When will she bring them?: Amanda: I baked  co...   \n",
       "3    When will she bring them?: Val: it's raining!\\...   \n",
       "4    Will Amanda bring Jerry some cookies?: Amanda:...   \n",
       "..                                                 ...   \n",
       "557  Is the weather in Poland getting cooler?: Adel...   \n",
       "558  Has Justin seen the last episode of Game of Th...   \n",
       "559  Has Justin seen the last episode of Game of Th...   \n",
       "560  Why can't Justin wait to see it?: Tom: Howâ€™s t...   \n",
       "561  Why can't Justin wait to see it?: Adele: i am ...   \n",
       "\n",
       "                                                answer  \n",
       "0                       Amanda baked cookies for Jerry  \n",
       "1               I cannot be sure given this transcript  \n",
       "2                         She will bring them tomorrow  \n",
       "3               I cannot be sure given this transcript  \n",
       "4                 Amanda will bring Jerry some cookies  \n",
       "..                                                 ...  \n",
       "557             I cannot be sure given this transcript  \n",
       "558  Justin hasn't seen the last episode of Game of...  \n",
       "559             I cannot be sure given this transcript  \n",
       "560                       Justin can't wait to see it.  \n",
       "561             I cannot be sure given this transcript  \n",
       "\n",
       "[562 rows x 3 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_df = pd.DataFrame(conv_data)\n",
    "\n",
    "conv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "09f90bc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2a795b730>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAGhCAYAAABLWk8IAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3df3RTdYL//1doQ9pCW2k7bRqpnergzmrBcVsHQUdQaBEFVDyLIzOKLp5lV0C7gCiwnAmrFGXOAG45ou5wAGF7ymfOUMddEQirFHt6WEuRlbKzypypCNrao1P6g9Y0tPf7xxzynZDwI21D32mfj3Nyat73nZv7fp1rfXmTNDbLsiwBAAAYZEh/HwAAAMCFKCgAAMA4FBQAAGAcCgoAADAOBQUAABiHggIAAIxDQQEAAMahoAAAAONQUAAAgHEoKAAAwDi9Kihr1qyRzWZTUVGRf8yyLLndbrlcLsXHx2vixIk6fvx4wOO8Xq8WLlyotLQ0DRs2TDNmzNDp06d7cygAAGAA6XFBqa6u1ptvvqkxY8YEjK9du1br1q3Txo0bVV1dLafTqYKCArW2tvrnFBUVqby8XGVlZaqsrFRbW5umTZumrq6unq8EAAAMGLaefFlgW1ub/uZv/kavvfaaXnrpJf3oRz/Shg0bZFmWXC6XioqK9Pzzz0v689WSjIwMvfLKK5o3b56am5v1ve99T9u3b9cjjzwiSfrqq6+UlZWl3bt3a8qUKZd9/u7ubn311VdKTEyUzWYL9/ABAEA/sCxLra2tcrlcGjLk0tdIYnvyBPPnz9f999+vyZMn66WXXvKP19XVqaGhQYWFhf4xh8OhCRMmqKqqSvPmzVNNTY18Pl/AHJfLpdzcXFVVVYUsKF6vV16v13//yy+/1E033dSTQwcAAP3s1KlTGjly5CXnhF1QysrKdOTIEVVXVwdta2hokCRlZGQEjGdkZOjkyZP+OUOHDtWIESOC5px//IXWrFmjVatWBY3/+te/VkJCQrhLAAAA/aC9vV1PPfWUEhMTLzs3rIJy6tQpPfvss9q3b5/i4uIuOu/Cl10sy7rsSzGXmrNs2TItWrTIf7+lpUVZWVl68MEHlZSUFMYKBg6fzyePx6OCggLZ7fb+PhwjkEkwMgmNXIKRSTAyCa03ubS0tOipp566ordnhFVQampq1NjYqLy8PP9YV1eXDh48qI0bN+rTTz+V9OerJJmZmf45jY2N/qsqTqdTnZ2dampqCriK0tjYqPHjx4d8XofDIYfDETRut9sH/UlDBsHIJBiZhEYuwcgkGJmE1pNcwpkf1qd4Jk2apGPHjuno0aP+W35+vn72s5/p6NGjuv766+V0OuXxePyP6ezsVEVFhb985OXlyW63B8ypr69XbW3tRQsKAAAYXMK6gpKYmKjc3NyAsWHDhik1NdU/XlRUpOLiYo0aNUqjRo1ScXGxEhISNHv2bElScnKy5s6dq8WLFys1NVUpKSlasmSJRo8ercmTJ/fRsgAAQDTr0ad4LmXp0qXq6OjQ008/raamJo0dO1b79u0LeEPM+vXrFRsbq1mzZqmjo0OTJk3S1q1bFRMT09eHAwAAolCvC8qBAwcC7ttsNrndbrnd7os+Ji4uTiUlJSopKent0wMAgAGI7+IBAADGoaAAAADjUFAAAIBxKCgAAMA4FBQAAGAcCgoAADAOBQUAABiHggIAAIxDQQEAAMbp8z91D1yp77/wbp/uzxFjae2PpVz3Xnm7Lv9V3j3x+cv3R2S/AIBAXEEBAADGoaAAAADjUFAAAIBxKCgAAMA4FBQAAGAcCgoAADAOBQUAABiHggIAAIxDQQEAAMahoAAAAONQUAAAgHEoKAAAwDgUFAAAYBwKCgAAMA4FBQAAGIeCAgAAjENBAQAAxqGgAAAA41BQAACAcSgoAADAOBQUAABgHAoKAAAwDgUFAAAYh4ICAACMQ0EBAADGCaugbNq0SWPGjFFSUpKSkpI0btw4vffee/7tTzzxhGw2W8Dt9ttvD9iH1+vVwoULlZaWpmHDhmnGjBk6ffp036wGAAAMCGEVlJEjR+rll1/W4cOHdfjwYd1zzz164IEHdPz4cf+ce++9V/X19f7b7t27A/ZRVFSk8vJylZWVqbKyUm1tbZo2bZq6urr6ZkUAACDqxYYzefr06QH3V69erU2bNunQoUO6+eabJUkOh0NOpzPk45ubm7V582Zt375dkydPliTt2LFDWVlZ2r9/v6ZMmdKTNQAAgAEmrILyl7q6uvSb3/xGZ8+e1bhx4/zjBw4cUHp6uq655hpNmDBBq1evVnp6uiSppqZGPp9PhYWF/vkul0u5ubmqqqq6aEHxer3yer3++y0tLZIkn88nn8/X0yVEtfPrjub1O2Ksvt3fECvgZyREW94D4TyJBHIJRibByCS03uQSzmNslmWF9dv82LFjGjdunL777jsNHz5cpaWluu+++yRJO3fu1PDhw5Wdna26ujqtXLlS586dU01NjRwOh0pLS/Xkk08GlA1JKiwsVE5Ojt54442Qz+l2u7Vq1aqg8dLSUiUkJIRz+AAAoJ+0t7dr9uzZam5uVlJS0iXnhl1QOjs79cUXX+jMmTP67W9/q1//+teqqKjQTTfdFDS3vr5e2dnZKisr08yZMy9aUAoKCnTDDTfo9ddfD/mcoa6gZGVl6ZtvvrnsAgcqn88nj8ejgoIC2e32/j6cHsl17+3T/TmGWHoxv1srDw+Rt9vWp/s+r9YdXS9DDoTzJBLIJRiZBCOT0HqTS0tLi9LS0q6ooIT9Es/QoUP1gx/8QJKUn5+v6upqvfrqqyGvfmRmZio7O1snTpyQJDmdTnV2dqqpqUkjRozwz2tsbNT48eMv+pwOh0MOhyNo3G63D/qTJpoz8HZFpkR4u20R23e0Zh3N50kkkUswMglGJqH1JJdw5vf676BYlhV0ReS8b7/9VqdOnVJmZqYkKS8vT3a7XR6Pxz+nvr5etbW1lywoAABgcAnrCsry5cs1depUZWVlqbW1VWVlZTpw4ID27NmjtrY2ud1uPfzww8rMzNTnn3+u5cuXKy0tTQ899JAkKTk5WXPnztXixYuVmpqqlJQULVmyRKNHj/Z/qgcAACCsgvL111/rscceU319vZKTkzVmzBjt2bNHBQUF6ujo0LFjx/TWW2/pzJkzyszM1N13362dO3cqMTHRv4/169crNjZWs2bNUkdHhyZNmqStW7cqJiamzxcHAACiU1gFZfPmzRfdFh8fr717L/+mx7i4OJWUlKikpCScpwYAAIMI38UDAACMQ0EBAADGoaAAAADjUFAAAIBxKCgAAMA4FBQAAGAcCgoAADAOBQUAABiHggIAAIxDQQEAAMahoAAAAONQUAAAgHEoKAAAwDgUFAAAYBwKCgAAMA4FBQAAGIeCAgAAjENBAQAAxqGgAAAA41BQAACAcSgoAADAOBQUAABgHAoKAAAwDgUFAAAYh4ICAACMQ0EBAADGoaAAAADjUFAAAIBxKCgAAMA4FBQAAGAcCgoAADAOBQUAABiHggIAAIxDQQEAAMahoAAAAOOEVVA2bdqkMWPGKCkpSUlJSRo3bpzee+89/3bLsuR2u+VyuRQfH6+JEyfq+PHjAfvwer1auHCh0tLSNGzYMM2YMUOnT5/um9UAAIABIayCMnLkSL388ss6fPiwDh8+rHvuuUcPPPCAv4SsXbtW69at08aNG1VdXS2n06mCggK1trb691FUVKTy8nKVlZWpsrJSbW1tmjZtmrq6uvp2ZQAAIGqFVVCmT5+u++67TzfeeKNuvPFGrV69WsOHD9ehQ4dkWZY2bNigFStWaObMmcrNzdW2bdvU3t6u0tJSSVJzc7M2b96sX/3qV5o8ebJuvfVW7dixQ8eOHdP+/fsjskAAABB9Ynv6wK6uLv3mN7/R2bNnNW7cONXV1amhoUGFhYX+OQ6HQxMmTFBVVZXmzZunmpoa+Xy+gDkul0u5ubmqqqrSlClTQj6X1+uV1+v1329paZEk+Xw++Xy+ni4hqp1fdzSv3xFj9e3+hlgBPyMh2vIeCOdJJJBLMDIJRiah9SaXcB4TdkE5duyYxo0bp++++07Dhw9XeXm5brrpJlVVVUmSMjIyAuZnZGTo5MmTkqSGhgYNHTpUI0aMCJrT0NBw0edcs2aNVq1aFTS+b98+JSQkhLuEAcXj8fT3IfTY2h9HZr8v5ndHZseSdu/eHbF9R1I0nyeRRC7ByCQYmYTWk1za29uveG7YBeWv/uqvdPToUZ05c0a//e1vNWfOHFVUVPi322y2gPmWZQWNXehyc5YtW6ZFixb577e0tCgrK0uFhYVKSkoKdwkDgs/nk8fjUUFBgex2e38fTo/kuvf26f4cQyy9mN+tlYeHyNt96XOup2rdoa/ymWognCeRQC7ByCQYmYTWm1zOvwJyJcIuKEOHDtUPfvADSVJ+fr6qq6v16quv6vnnn5f056skmZmZ/vmNjY3+qypOp1OdnZ1qamoKuIrS2Nio8ePHX/Q5HQ6HHA5H0Ljdbh/0J000Z+DtikyJ8HbbIrbvaM06ms+TSCKXYGQSjExC60ku4czv9d9BsSxLXq9XOTk5cjqdAZd8Ojs7VVFR4S8feXl5stvtAXPq6+tVW1t7yYICAAAGl7CuoCxfvlxTp05VVlaWWltbVVZWpgMHDmjPnj2y2WwqKipScXGxRo0apVGjRqm4uFgJCQmaPXu2JCk5OVlz587V4sWLlZqaqpSUFC1ZskSjR4/W5MmTI7JAAAAQfcIqKF9//bUee+wx1dfXKzk5WWPGjNGePXtUUFAgSVq6dKk6Ojr09NNPq6mpSWPHjtW+ffuUmJjo38f69esVGxurWbNmqaOjQ5MmTdLWrVsVExPTtysDAABRK6yCsnnz5ktut9lscrvdcrvdF50TFxenkpISlZSUhPPUAABgEOG7eAAAgHEoKAAAwDgUFAAAYBwKCgAAMA4FBQAAGIeCAgAAjENBAQAAxqGgAAAA41BQAACAcSgoAADAOBQUAABgHAoKAAAwDgUFAAAYh4ICAACMQ0EBAADGoaAAAADjUFAAAIBxKCgAAMA4FBQAAGAcCgoAADAOBQUAABiHggIAAIxDQQEAAMahoAAAAONQUAAAgHEoKAAAwDgUFAAAYBwKCgAAMA4FBQAAGIeCAgAAjENBAQAAxqGgAAAA41BQAACAcSgoAADAOBQUAABgnLAKypo1a3TbbbcpMTFR6enpevDBB/Xpp58GzHniiSdks9kCbrfffnvAHK/Xq4ULFyotLU3Dhg3TjBkzdPr06d6vBgAADAhhFZSKigrNnz9fhw4dksfj0blz51RYWKizZ88GzLv33ntVX1/vv+3evTtge1FRkcrLy1VWVqbKykq1tbVp2rRp6urq6v2KAABA1IsNZ/KePXsC7m/ZskXp6emqqanRXXfd5R93OBxyOp0h99Hc3KzNmzdr+/btmjx5siRpx44dysrK0v79+zVlypRw1wAAAAaYsArKhZqbmyVJKSkpAeMHDhxQenq6rrnmGk2YMEGrV69Wenq6JKmmpkY+n0+FhYX++S6XS7m5uaqqqgpZULxer7xer/9+S0uLJMnn88nn8/VmCVHr/Lqjef2OGKtv9zfECvgZCdGW90A4TyKBXIKRSTAyCa03uYTzGJtlWT36bW5Zlh544AE1NTXpww8/9I/v3LlTw4cPV3Z2turq6rRy5UqdO3dONTU1cjgcKi0t1ZNPPhlQOCSpsLBQOTk5euONN4Key+12a9WqVUHjpaWlSkhI6MnhAwCAq6y9vV2zZ89Wc3OzkpKSLjm3x1dQFixYoE8++USVlZUB44888oj/n3Nzc5Wfn6/s7Gy9++67mjlz5kX3Z1mWbDZbyG3Lli3TokWL/PdbWlqUlZWlwsLCyy5woPL5fPJ4PCooKJDdbu/vw+mRXPfePt2fY4ilF/O7tfLwEHm7Q59LvVXrjq6XIAfCeRIJ5BKMTIKRSWi9yeX8KyBXokcFZeHChXrnnXd08OBBjRw58pJzMzMzlZ2drRMnTkiSnE6nOjs71dTUpBEjRvjnNTY2avz48SH34XA45HA4gsbtdvugP2miOQNvV2RKhLfbFrF9R2vW0XyeRBK5BCOTYGQSWk9yCWd+WJ/isSxLCxYs0K5du/T+++8rJyfnso/59ttvderUKWVmZkqS8vLyZLfb5fF4/HPq6+tVW1t70YICAAAGl7CuoMyfP1+lpaX63e9+p8TERDU0NEiSkpOTFR8fr7a2Nrndbj388MPKzMzU559/ruXLlystLU0PPfSQf+7cuXO1ePFipaamKiUlRUuWLNHo0aP9n+oBAACDW1gFZdOmTZKkiRMnBoxv2bJFTzzxhGJiYnTs2DG99dZbOnPmjDIzM3X33Xdr586dSkxM9M9fv369YmNjNWvWLHV0dGjSpEnaunWrYmJier8iAAAQ9cIqKJf7wE98fLz27r38Gx/j4uJUUlKikpKScJ4eAAAMEnwXDwAAMA4FBQAAGIeCAgAAjENBAQAAxqGgAAAA41BQAACAcSgoAADAOBQUAABgHAoKAAAwDgUFAAAYh4ICAACMQ0EBAADGoaAAAADjUFAAAIBxKCgAAMA4FBQAAGAcCgoAADAOBQUAABiHggIAAIxDQQEAAMahoAAAAONQUAAAgHEoKAAAwDgUFAAAYBwKCgAAMA4FBQAAGIeCAgAAjENBAQAAxqGgAAAA41BQAACAcSgoAADAOBQUAABgHAoKAAAwDgUFAAAYh4ICAACME1ZBWbNmjW677TYlJiYqPT1dDz74oD799NOAOZZlye12y+VyKT4+XhMnTtTx48cD5ni9Xi1cuFBpaWkaNmyYZsyYodOnT/d+NQAAYEAIq6BUVFRo/vz5OnTokDwej86dO6fCwkKdPXvWP2ft2rVat26dNm7cqOrqajmdThUUFKi1tdU/p6ioSOXl5SorK1NlZaXa2to0bdo0dXV19d3KAABA1IoNZ/KePXsC7m/ZskXp6emqqanRXXfdJcuytGHDBq1YsUIzZ86UJG3btk0ZGRkqLS3VvHnz1NzcrM2bN2v79u2aPHmyJGnHjh3KysrS/v37NWXKlD5aGgAAiFZhFZQLNTc3S5JSUlIkSXV1dWpoaFBhYaF/jsPh0IQJE1RVVaV58+appqZGPp8vYI7L5VJubq6qqqpCFhSv1yuv1+u/39LSIkny+Xzy+Xy9WULUOr/uaF6/I8bq2/0NsQJ+RkK05T0QzpNIIJdgZBKMTELrTS7hPKbHBcWyLC1atEh33nmncnNzJUkNDQ2SpIyMjIC5GRkZOnnypH/O0KFDNWLEiKA55x9/oTVr1mjVqlVB4/v27VNCQkJPlzAgeDye/j6EHlv748js98X87sjsWNLu3bsjtu9IiubzJJLIJRiZBCOT0HqSS3t7+xXP7XFBWbBggT755BNVVlYGbbPZbAH3LcsKGrvQpeYsW7ZMixYt8t9vaWlRVlaWCgsLlZSU1IOjj34+n08ej0cFBQWy2+39fTg9kuve26f7cwyx9GJ+t1YeHiJv96XPt56qdUfXS5AD4TyJBHIJRibByCS03uRy/hWQK9GjgrJw4UK98847OnjwoEaOHOkfdzqdkv58lSQzM9M/3tjY6L+q4nQ61dnZqaampoCrKI2NjRo/fnzI53M4HHI4HEHjdrt90J800ZyBtysyJcLbbYvYvqM162g+TyKJXIKRSTAyCa0nuYQzP6xP8ViWpQULFmjXrl16//33lZOTE7A9JydHTqcz4LJPZ2enKioq/OUjLy9Pdrs9YE59fb1qa2svWlAAAMDgEtYVlPnz56u0tFS/+93vlJiY6H/PSHJysuLj42Wz2VRUVKTi4mKNGjVKo0aNUnFxsRISEjR79mz/3Llz52rx4sVKTU1VSkqKlixZotGjR/s/1QMAAAa3sArKpk2bJEkTJ04MGN+yZYueeOIJSdLSpUvV0dGhp59+Wk1NTRo7dqz27dunxMRE//z169crNjZWs2bNUkdHhyZNmqStW7cqJiamd6sBAAADQlgFxbIu//FNm80mt9stt9t90TlxcXEqKSlRSUlJOE8PAAAGCb6LBwAAGIeCAgAAjENBAQAAxqGgAAAA41BQAACAcSgoAADAOBQUAABgHAoKAAAwDgUFAAAYh4ICAACMQ0EBAADGoaAAAADjUFAAAIBxKCgAAMA4FBQAAGAcCgoAADAOBQUAABiHggIAAIxDQQEAAMahoAAAAONQUAAAgHEoKAAAwDgUFAAAYBwKCgAAMA4FBQAAGIeCAgAAjENBAQAAxqGgAAAA41BQAACAcSgoAADAOBQUAABgHAoKAAAwDgUFAAAYh4ICAACMQ0EBAADGCbugHDx4UNOnT5fL5ZLNZtPbb78dsP2JJ56QzWYLuN1+++0Bc7xerxYuXKi0tDQNGzZMM2bM0OnTp3u3EgAAMGCEXVDOnj2rW265RRs3brzonHvvvVf19fX+2+7duwO2FxUVqby8XGVlZaqsrFRbW5umTZumrq6u8FcAAAAGnNhwHzB16lRNnTr1knMcDoecTmfIbc3Nzdq8ebO2b9+uyZMnS5J27NihrKws7d+/X1OmTAn3kAAAwAATdkG5EgcOHFB6erquueYaTZgwQatXr1Z6erokqaamRj6fT4WFhf75LpdLubm5qqqqCllQvF6vvF6v/35LS4skyefzyefzRWIJxju/7mhevyPG6tv9DbECfkZCtOU9EM6TSCCXYGQSjExC600u4TzGZllWj3+b22w2lZeX68EHH/SP7dy5U8OHD1d2drbq6uq0cuVKnTt3TjU1NXI4HCotLdWTTz4ZUDgkqbCwUDk5OXrjjTeCnsftdmvVqlVB46WlpUpISOjp4QMAgKuovb1ds2fPVnNzs5KSki45t8+voDzyyCP+f87NzVV+fr6ys7P17rvvaubMmRd9nGVZstlsIbctW7ZMixYt8t9vaWlRVlaWCgsLL7vAgcrn88nj8aigoEB2u72/D6dHct17+3R/jiGWXszv1srDQ+TtDn0u9VatO7peghwI50kkkEswMglGJqH1Jpfzr4BciYi8xPOXMjMzlZ2drRMnTkiSnE6nOjs71dTUpBEjRvjnNTY2avz48SH34XA45HA4gsbtdvugP2miOQNvV2RKhLfbFrF9R2vW0XyeRBK5BCOTYGQSWk9yCWd+xP8OyrfffqtTp04pMzNTkpSXlye73S6Px+OfU19fr9ra2osWFAAAMLiEfQWlra1Nf/jDH/z36+rqdPToUaWkpCglJUVut1sPP/ywMjMz9fnnn2v58uVKS0vTQw89JElKTk7W3LlztXjxYqWmpiolJUVLlizR6NGj/Z/qAQAAg1vYBeXw4cO6++67/ffPvzdkzpw52rRpk44dO6a33npLZ86cUWZmpu6++27t3LlTiYmJ/sesX79esbGxmjVrljo6OjRp0iRt3bpVMTExfbAkAAAQ7cIuKBMnTtSlPvizd+/l3/gYFxenkpISlZSUhPv0AABgEOC7eAAAgHEoKAAAwDgUFAAAYBwKCgAAMA4FBQAAGIeCAgAAjENBAQAAxqGgAAAA41BQAACAcSgoAADAOBQUAABgHAoKAAAwDgUFAAAYh4ICAACMQ0EBAADGoaAAAADjUFAAAIBxKCgAAMA4FBQAAGAcCgoAADAOBQUAABiHggIAAIxDQQEAAMahoAAAAONQUAAAgHEoKAAAwDgUFAAAYBwKCgAAMA4FBQAAGIeCAgAAjENBAQAAxqGgAAAA41BQAACAcSgoAADAOBQUAABgnLALysGDBzV9+nS5XC7ZbDa9/fbbAdsty5Lb7ZbL5VJ8fLwmTpyo48ePB8zxer1auHCh0tLSNGzYMM2YMUOnT5/u3UoAAMCAEXZBOXv2rG655RZt3Lgx5Pa1a9dq3bp12rhxo6qrq+V0OlVQUKDW1lb/nKKiIpWXl6usrEyVlZVqa2vTtGnT1NXV1fOVAACAASM23AdMnTpVU6dODbnNsixt2LBBK1as0MyZMyVJ27ZtU0ZGhkpLSzVv3jw1Nzdr8+bN2r59uyZPnixJ2rFjh7KysrR//35NmTKlF8sBAAADQdgF5VLq6urU0NCgwsJC/5jD4dCECRNUVVWlefPmqaamRj6fL2COy+VSbm6uqqqqQhYUr9crr9frv9/S0iJJ8vl88vl8fbmEqHF+3dG8fkeM1bf7G2IF/IyEaMt7IJwnkUAuwcgkGJmE1ptcwnlMnxaUhoYGSVJGRkbAeEZGhk6ePOmfM3ToUI0YMSJozvnHX2jNmjVatWpV0Pi+ffuUkJDQF4cetTweT38fQo+t/XFk9vtifndkdixp9+7dEdt3JEXzeRJJ5BKMTIKRSWg9yaW9vf2K5/ZpQTnPZrMF3LcsK2jsQpeas2zZMi1atMh/v6WlRVlZWSosLFRSUlLvDzgK+Xw+eTweFRQUyG639/fh9Eiue2+f7s8xxNKL+d1aeXiIvN2XPt96qtYdXS9BDoTzJBLIJRiZBCOT0HqTy/lXQK5EnxYUp9Mp6c9XSTIzM/3jjY2N/qsqTqdTnZ2dampqCriK0tjYqPHjx4fcr8PhkMPhCBq32+2D/qSJ5gy8XZEpEd5uW8T2Ha1ZR/N5EknkEoxMgpFJaD3JJZz5ffp3UHJycuR0OgMu+3R2dqqiosJfPvLy8mS32wPm1NfXq7a29qIFBQAADC5hX0Fpa2vTH/7wB//9uro6HT16VCkpKbruuutUVFSk4uJijRo1SqNGjVJxcbESEhI0e/ZsSVJycrLmzp2rxYsXKzU1VSkpKVqyZIlGjx7t/1QPAAAY3MIuKIcPH9bdd9/tv3/+vSFz5szR1q1btXTpUnV0dOjpp59WU1OTxo4dq3379ikxMdH/mPXr1ys2NlazZs1SR0eHJk2apK1btyomJqYPlgQAAKJd2AVl4sSJsqyLf4zTZrPJ7XbL7XZfdE5cXJxKSkpUUlIS7tMDAIBBgO/iAQAAxqGgAAAA41BQAACAcSgoAADAOBQUAABgHAoKAAAwDgUFAAAYh4ICAACMQ0EBAADGoaAAAADjUFAAAIBxKCgAAMA4FBQAAGAcCgoAADAOBQUAAOvt8iMAAA7/SURBVBiHggIAAIxDQQEAAMahoAAAAONQUAAAgHEoKAAAwDgUFAAAYBwKCgAAMA4FBQAAGIeCAgAAjENBAQAAxont7wNA3/j+C+/29yEAANBnuIICAACMQ0EBAADGoaAAAADjUFAAAIBxKCgAAMA4FBQAAGAcCgoAADAOBQUAABiHggIAAIxDQQEAAMbp84Lidrtls9kCbk6n07/dsiy53W65XC7Fx8dr4sSJOn78eF8fBgAAiGIRuYJy8803q76+3n87duyYf9vatWu1bt06bdy4UdXV1XI6nSooKFBra2skDgUAAEShiHxZYGxsbMBVk/Msy9KGDRu0YsUKzZw5U5K0bds2ZWRkqLS0VPPmzQu5P6/XK6/X67/f0tIiSfL5fPL5fBFYgfnOr/v8T0eM1Z+HYwTHECvgZyRE2/l24XmCPyOXYGQSjExC600u4TzGZllWn/42d7vd+uUvf6nk5GQ5HA6NHTtWxcXFuv766/XHP/5RN9xwg44cOaJbb73V/5gHHnhA11xzjbZt23bRfa5atSpovLS0VAkJCX15+AAAIELa29s1e/ZsNTc3Kykp6ZJz+7ygvPfee2pvb9eNN96or7/+Wi+99JL+7//+T8ePH9enn36qO+64Q19++aVcLpf/MX//93+vkydPau/evSH3GeoKSlZWlr755pvLLnCg8vl88ng8KigokN1uV647dHaDiWOIpRfzu7Xy8BB5u20ReY5a95SI7DdSLjxP8GfkEoxMgpFJaL3JpaWlRWlpaVdUUPr8JZ6pU6f6/3n06NEaN26cbrjhBm3btk233367JMlmC/yPh2VZQWN/yeFwyOFwBI3b7fZBf9Kcz8DbFZn/IEcjb7ctYnlE6/nGvyuhkUswMglGJqH1JJdw5kf8Y8bDhg3T6NGjdeLECf/7UhoaGgLmNDY2KiMjI9KHAgAAokRE3iT7l7xer37/+9/rJz/5iXJycuR0OuXxePzvQens7FRFRYVeeeWVSB8K0Gvff+Hd/j6EsDhiLK39cX8fBQCEr88LypIlSzR9+nRdd911amxs1EsvvaSWlhbNmTNHNptNRUVFKi4u1qhRozRq1CgVFxcrISFBs2fP7utDAQAAUarPC8rp06f16KOP6ptvvtH3vvc93X777Tp06JCys7MlSUuXLlVHR4eefvppNTU1aezYsdq3b58SExP7+lAAAECU6vOCUlZWdsntNptNbrdbbre7r58aAAAMEHwXDwAAMA4FBQAAGIeCAgAAjENBAQAAxqGgAAAA41BQAACAcSgoAADAOBQUAABgHAoKAAAwDgUFAAAYh4ICAACMQ0EBAADGoaAAAADjUFAAAIBxKCgAAMA4sf19AAAiL9e9V94uW38fxhX7/OX7+/sQAPQzrqAAAADjUFAAAIBxKCgAAMA4vAcFgHG+/8K7Ed2/I8bS2h/37XtzeN8M0Le4ggIAAIxDQQEAAMahoAAAAONQUAAAgHEoKAAAwDgUFAAAYBwKCgAAMA4FBQAAGIeCAgAAjMNfkg0h0n/Fsi9E4i9hAgBgCq6gAAAA41BQAACAcSgoAADAOLwHBQAGsfPvuYum97XxzdGDAwUFAPpANLy5Hogm/foSz2uvvaacnBzFxcUpLy9PH374YX8eDgAAMES/FZSdO3eqqKhIK1as0Mcff6yf/OQnmjp1qr744ov+OiQAAGCIfnuJZ926dZo7d66eeuopSdKGDRu0d+9ebdq0SWvWrAmY6/V65fV6/febm5slSX/605/k8/n6/Nhiz53t8332tdhuS+3t3Yr1DVFXt9mvF18tZBKMTEIjl2DRlMkPlvy/q/I8jiGW/vnWbv1oxS55Dc8kEv572aSQ4z6fT+3t7fr2229lt9vD2mdra6skybKsy0+2+oHX67ViYmKsXbt2BYw/88wz1l133RU0/xe/+IUliRs3bty4ceM2AG6nTp26bFfolyso33zzjbq6upSRkREwnpGRoYaGhqD5y5Yt06JFi/z3u7u79ac//Umpqamy2QZfq5WklpYWZWVl6dSpU0pKSurvwzECmQQjk9DIJRiZBCOT0HqTi2VZam1tlcvluuzcfv0Uz4XlwrKskIXD4XDI4XAEjF1zzTURPbZokZSUxL84FyCTYGQSGrkEI5NgZBJaT3NJTk6+onn98ibZtLQ0xcTEBF0taWxsDLqqAgAABp9+KShDhw5VXl6ePB5PwLjH49H48eP745AAAIBBYtxut7s/njgpKUkrV67Utddeq7i4OBUXF+uDDz7Qli1bePnmCsXExGjixImKjeXv7Z1HJsHIJDRyCUYmwcgktKuRi82yruSzPpHx2muvae3ataqvr1dubq7Wr1+vu+66q78OBwAAGKJfCwoAAEAofJsxAAAwDgUFAAAYh4ICAACMQ0EBAADGoaAYbM2aNbrtttuUmJio9PR0Pfjgg/r0008D5liWJbfbLZfLpfj4eE2cOFHHjx/vpyO++tasWSObzaaioiL/2GDM5Msvv9TPf/5zpaamKiEhQT/60Y9UU1Pj3z4YMzl37pz++Z//WTk5OYqPj9f111+vf/mXf1F3d7d/zkDP5eDBg5o+fbpcLpdsNpvefvvtgO1Xsn6v16uFCxcqLS1Nw4YN04wZM3T69OmruYw+d6lcfD6fnn/+eY0ePVrDhg2Ty+XS448/rq+++ipgHwMtl8udK39p3rx5stls2rBhQ8B4X2dCQTFYRUWF5s+fr0OHDsnj8ejcuXMqLCzU2bP//7ctr127VuvWrdPGjRtVXV0tp9OpgoIC/zdGDmTV1dV68803NWbMmIDxwZZJU1OT7rjjDtntdr333nv63//9X/3qV78K+HtCgy0TSXrllVf0+uuva+PGjfr973+vtWvX6pe//KVKSkr8cwZ6LmfPntUtt9yijRs3htx+JesvKipSeXm5ysrKVFlZqba2Nk2bNk1dXV1Xaxl97lK5tLe368iRI1q5cqWOHDmiXbt26bPPPtOMGTMC5g20XC53rpz39ttv67//+79DfpdOn2fSu+8lxtXU2NhoSbIqKiosy7Ks7u5uy+l0Wi+//LJ/znfffWclJydbr7/+en8d5lXR2tpqjRo1yvJ4PNaECROsZ5991rKswZnJ888/b915550X3T4YM7Esy7r//vutv/u7vwsYmzlzpvXzn//csqzBl4skq7y83H//StZ/5swZy263W2VlZf45X375pTVkyBBrz549V+/gI+jCXEL56KOPLEnWyZMnLcsa+LlcLJPTp09b1157rVVbW2tlZ2db69ev92+LRCZcQYkizc3NkqSUlBRJUl1dnRoaGlRYWOif43A4NGHCBFVVVfXLMV4t8+fP1/3336/JkycHjA/GTN555x3l5+frb//2b5Wenq5bb71V//Zv/+bfPhgzkaQ777xT//Vf/6XPPvtMkvQ///M/qqys1H333Sdp8OZy3pWsv6amRj6fL2COy+VSbm7uoMjovObmZtlsNv9VycGYS3d3tx577DE999xzuvnmm4O2RyIT/nZvlLAsS4sWLdKdd96p3NxcSfJ/2eKFX7CYkZGhkydPXvVjvFrKysp05MgRVVdXB20bjJn88Y9/1KZNm7Ro0SItX75cH330kZ555hk5HA49/vjjgzITSXr++efV3NysH/7wh4qJiVFXV5dWr16tRx99VNLgPFf+0pWsv6GhQUOHDtWIESOC5lz4Za8D1XfffacXXnhBs2fP9n9z72DM5ZVXXlFsbKyeeeaZkNsjkQkFJUosWLBAn3zyiSorK4O22Wy2gPuWZQWNDRSnTp3Ss88+q3379ikuLu6i8wZTJt3d3crPz1dxcbEk6dZbb9Xx48e1adMmPf744/55gykTSdq5c6d27Nih0tJS3XzzzTp69KiKiorkcrk0Z84c/7zBlsuFerL+wZKRz+fTT3/6U3V3d+u111677PyBmktNTY1effVVHTlyJOz19SYTXuKJAgsXLtQ777yjDz74QCNHjvSPO51OSQpqp42NjUH/VzRQ1NTUqLGxUXl5eYqNjVVsbKwqKir0r//6r4qNjfWvezBlkpmZqZtuuilg7K//+q/1xRdfSBqc54kkPffcc3rhhRf005/+VKNHj9Zjjz2mf/qnf9KaNWskDd5czruS9TudTnV2dqqpqemicwYqn8+nWbNmqa6uTh6Px3/1RBp8uXz44YdqbGzUdddd5/+9e/LkSS1evFjf//73JUUmEwqKwSzL0oIFC7Rr1y69//77ysnJCdiek5Mjp9Mpj8fjH+vs7FRFRYXGjx9/tQ/3qpg0aZKOHTumo0eP+m/5+fn62c9+pqNHj+r6668fdJnccccdQR8//+yzz5SdnS1pcJ4n0p8/jTFkSOCvuJiYGP/HjAdrLuddyfrz8vJkt9sD5tTX16u2tnZAZ3S+nJw4cUL79+9XampqwPbBlstjjz2mTz75JOD3rsvl0nPPPae9e/dKilAmPXprLa6Kf/zHf7SSk5OtAwcOWPX19f5be3u7f87LL79sJScnW7t27bKOHTtmPfroo1ZmZqbV0tLSj0d+df3lp3gsa/Bl8tFHH1mxsbHW6tWrrRMnTlj//u//biUkJFg7duzwzxlsmViWZc2ZM8e69tprrf/8z/+06urqrF27dllpaWnW0qVL/XMGei6tra3Wxx9/bH388ceWJGvdunXWxx9/7P80ypWs/x/+4R+skSNHWvv377eOHDli3XPPPdYtt9xinTt3rr+W1WuXysXn81kzZsywRo4caR09ejTgd6/X6/XvY6Dlcrlz5UIXforHsvo+EwqKwSSFvG3ZssU/p7u72/rFL35hOZ1Oy+FwWHfddZd17Nix/jvofnBhQRmMmfzHf/yHlZubazkcDuuHP/yh9eabbwZsH4yZtLS0WM8++6x13XXXWXFxcdb1119vrVixIuA/MgM9lw8++CDk75A5c+ZYlnVl6+/o6LAWLFhgpaSkWPHx8da0adOsL774oh9W03culUtdXd1Ff/d+8MEH/n0MtFwud65cKFRB6etMbJZlWT279gIAABAZvAcFAAAYh4ICAACMQ0EBAADGoaAAAADjUFAAAIBxKCgAAMA4FBQAAGAcCgoAADAOBQUAABiHggIAAIxDQQEAAMb5/wBEjWgS+SH//gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conv_df['answer'].str.len().hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a2d692f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(554, 3)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_df = conv_df[conv_df['question'].str.len() >= 20]\n",
    "conv_df = conv_df[conv_df['answer'].str.len() >= 20]\n",
    "\n",
    "conv_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "68aebab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_df.to_csv('../data/conversation_qa.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5335c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10cd358",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1564a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724dcdc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "9e5e9ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_df = pd.read_csv('../data/conversation_qa.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "3ed27055",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_qa_dataset = Dataset.from_pandas(conv_df)\n",
    "\n",
    "conv_qa_dataset = conv_qa_dataset.train_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "beb506fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'prompt', 'answer'],\n",
       "        num_rows: 415\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'prompt', 'answer'],\n",
       "        num_rows: 139\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_qa_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "a0c5c3f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b86512d77e8d47eeb8ab40a9b6bd7328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ce353b9d2224dbd875e33b6a91159d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(sample):\n",
    "    # add prefix to the input for t5\n",
    "\n",
    "    # tokenize inputs\n",
    "    model_inputs = tokenizer(sample[\"prompt\"], truncation=True)\n",
    "\n",
    "    # Tokenize targets with the `text_target` keyword argument\n",
    "    labels = tokenizer(text_target=sample[\"answer\"], truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "conv_qa_dataset['tokenized_train'] = conv_qa_dataset['train'].map(preprocess_function, batched=True, remove_columns=[\"question\", \"prompt\", \"answer\"])\n",
    "conv_qa_dataset['tokenized_test'] = conv_qa_dataset['test'].map(preprocess_function, batched=True, remove_columns=[\"question\", \"prompt\", \"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "8651ee24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'prompt', 'answer'],\n",
       "        num_rows: 415\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'prompt', 'answer'],\n",
       "        num_rows: 139\n",
       "    })\n",
       "    tokenized_train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 415\n",
       "    })\n",
       "    tokenized_test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 139\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_qa_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc416e9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "86d56fe2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/sinanozdemir/.cache/huggingface/hub/models--philschmid--flan-t5-base-samsum/snapshots/dab5b3234360d786065ab29c6dd5af00a0bff05f/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"philschmid/flan-t5-base-samsum\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /Users/sinanozdemir/.cache/huggingface/hub/models--philschmid--flan-t5-base-samsum/snapshots/dab5b3234360d786065ab29c6dd5af00a0bff05f/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at philschmid/flan-t5-base-samsum.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"philschmid/flan-t5-base-samsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1fcb8b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "task = 'conv_qa'\n",
    "repository_id = f\"{model_id.split('/')[1]}-{dataset_id}-{task}\"\n",
    "\n",
    "# Define training args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=repository_id,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    predict_with_generate=False,\n",
    "    fp16=False, # Overflows with fp16\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=10,\n",
    "    # logging & evaluation strategies\n",
    "    logging_dir=f\"{repository_id}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=conv_qa_dataset[\"tokenized_train\"],\n",
    "    eval_dataset=conv_qa_dataset[\"tokenized_test\"],\n",
    "    data_collator=data_collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f833bd71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 139\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 09:25]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 31.164133071899414,\n",
       " 'eval_runtime': 24.0401,\n",
       " 'eval_samples_per_second': 5.782,\n",
       " 'eval_steps_per_second': 0.208}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0c78a916",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 415\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 260\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='260' max='260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [260/260 1:33:02, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6.808400</td>\n",
       "      <td>6.813702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.010200</td>\n",
       "      <td>2.525110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.899400</td>\n",
       "      <td>1.351636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.880100</td>\n",
       "      <td>0.532311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.605100</td>\n",
       "      <td>0.236293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.402500</td>\n",
       "      <td>0.154330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.212400</td>\n",
       "      <td>0.122804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.202300</td>\n",
       "      <td>0.114072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.217500</td>\n",
       "      <td>0.108529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.169300</td>\n",
       "      <td>0.105540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 139\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to flan-t5-small-samsum-conv_qa/checkpoint-26\n",
      "Configuration saved in flan-t5-small-samsum-conv_qa/checkpoint-26/config.json\n",
      "Model weights saved in flan-t5-small-samsum-conv_qa/checkpoint-26/pytorch_model.bin\n",
      "Deleting older checkpoint [flan-t5-small-samsum-conv_qa/checkpoint-104] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 139\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to flan-t5-small-samsum-conv_qa/checkpoint-52\n",
      "Configuration saved in flan-t5-small-samsum-conv_qa/checkpoint-52/config.json\n",
      "Model weights saved in flan-t5-small-samsum-conv_qa/checkpoint-52/pytorch_model.bin\n",
      "Deleting older checkpoint [flan-t5-small-samsum-conv_qa/checkpoint-130] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 139\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to flan-t5-small-samsum-conv_qa/checkpoint-78\n",
      "Configuration saved in flan-t5-small-samsum-conv_qa/checkpoint-78/config.json\n",
      "Model weights saved in flan-t5-small-samsum-conv_qa/checkpoint-78/pytorch_model.bin\n",
      "Deleting older checkpoint [flan-t5-small-samsum-conv_qa/checkpoint-26] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 139\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to flan-t5-small-samsum-conv_qa/checkpoint-104\n",
      "Configuration saved in flan-t5-small-samsum-conv_qa/checkpoint-104/config.json\n",
      "Model weights saved in flan-t5-small-samsum-conv_qa/checkpoint-104/pytorch_model.bin\n",
      "Deleting older checkpoint [flan-t5-small-samsum-conv_qa/checkpoint-52] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 139\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to flan-t5-small-samsum-conv_qa/checkpoint-130\n",
      "Configuration saved in flan-t5-small-samsum-conv_qa/checkpoint-130/config.json\n",
      "Model weights saved in flan-t5-small-samsum-conv_qa/checkpoint-130/pytorch_model.bin\n",
      "Deleting older checkpoint [flan-t5-small-samsum-conv_qa/checkpoint-78] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 139\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to flan-t5-small-samsum-conv_qa/checkpoint-156\n",
      "Configuration saved in flan-t5-small-samsum-conv_qa/checkpoint-156/config.json\n",
      "Model weights saved in flan-t5-small-samsum-conv_qa/checkpoint-156/pytorch_model.bin\n",
      "Deleting older checkpoint [flan-t5-small-samsum-conv_qa/checkpoint-104] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 139\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to flan-t5-small-samsum-conv_qa/checkpoint-182\n",
      "Configuration saved in flan-t5-small-samsum-conv_qa/checkpoint-182/config.json\n",
      "Model weights saved in flan-t5-small-samsum-conv_qa/checkpoint-182/pytorch_model.bin\n",
      "Deleting older checkpoint [flan-t5-small-samsum-conv_qa/checkpoint-130] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 139\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to flan-t5-small-samsum-conv_qa/checkpoint-208\n",
      "Configuration saved in flan-t5-small-samsum-conv_qa/checkpoint-208/config.json\n",
      "Model weights saved in flan-t5-small-samsum-conv_qa/checkpoint-208/pytorch_model.bin\n",
      "Deleting older checkpoint [flan-t5-small-samsum-conv_qa/checkpoint-156] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 139\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to flan-t5-small-samsum-conv_qa/checkpoint-234\n",
      "Configuration saved in flan-t5-small-samsum-conv_qa/checkpoint-234/config.json\n",
      "Model weights saved in flan-t5-small-samsum-conv_qa/checkpoint-234/pytorch_model.bin\n",
      "Deleting older checkpoint [flan-t5-small-samsum-conv_qa/checkpoint-182] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 139\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to flan-t5-small-samsum-conv_qa/checkpoint-260\n",
      "Configuration saved in flan-t5-small-samsum-conv_qa/checkpoint-260/config.json\n",
      "Model weights saved in flan-t5-small-samsum-conv_qa/checkpoint-260/pytorch_model.bin\n",
      "Deleting older checkpoint [flan-t5-small-samsum-conv_qa/checkpoint-208] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from flan-t5-small-samsum-conv_qa/checkpoint-260 (score: 0.10554023087024689).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=260, training_loss=2.5851049108574022, metrics={'train_runtime': 5594.4917, 'train_samples_per_second': 0.742, 'train_steps_per_second': 0.046, 'total_flos': 2254570270018560.0, 'train_loss': 2.5851049108574022, 'epoch': 10.0})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9d00203c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 139\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.10554023087024689,\n",
       " 'eval_runtime': 22.4092,\n",
       " 'eval_samples_per_second': 6.203,\n",
       " 'eval_steps_per_second': 0.223,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "972f4e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to flan-t5-small-samsum-conv_qa\n",
      "Configuration saved in flan-t5-small-samsum-conv_qa/config.json\n",
      "Model weights saved in flan-t5-small-samsum-conv_qa/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4936fd1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "f7cb9514",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file flan-t5-small-samsum-conv_qa/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"flan-t5-small-samsum-conv_qa\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file flan-t5-small-samsum-conv_qa/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at flan-t5-small-samsum-conv_qa.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Load the model later\n",
    "\n",
    "finetuned_qa_model = AutoModelForSeq2SeqLM.from_pretrained(repository_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "a7ec97eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dialogue: \n",
      "Why isn't Monica interested in buying a day spa gift card for Sophia?: Sophia: What should I get them?\r\n",
      "Monica: How should I know?\r\n",
      "Sophia: come on help me out\r\n",
      "Monica: I dont know really Im bad at buying gifts\r\n",
      "Sophia: maybe some day spa gift card\r\n",
      "Monica: maybe\r\n",
      "Sophia: youre not really helpful :P\r\n",
      "Monica: Told ya... bad at gifts\n",
      "---------------\n",
      "question: \n",
      "Why isn't Monica interested in buying a day spa gift card for Sophia?\n",
      "---------------\n",
      "actual answer: \n",
      "Sophia wants to buy a day spa gift card, but Monica isn't interested in it\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "sample = conv_qa_dataset['test'][randrange(len(conv_qa_dataset[\"test\"]))]\n",
    "print(f\"dialogue: \\n{sample['prompt']}\\n---------------\")\n",
    "question = sample['prompt'].split(': ')[0]\n",
    "print(f\"question: \\n{question}\\n---------------\")\n",
    "print(f\"actual answer: \\n{sample['answer']}\\n---------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "bbd1a584",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fine-tuned flan-t5 answer:\n",
      "Monica isn't interested in buying a day spa gift card for Sophia.\n",
      "Monica isn't interested in buying a day spa gift card for Sophia.\n",
      "Monica isn't interested in buying a day spa gift card for Sophia.\n",
      "Monica isn't interested in buying a day spa gift card for Sophia.\n",
      "Monica isn't interested in buying a day spa gift card for Sophia.\n"
     ]
    }
   ],
   "source": [
    "_in = tokenizer.encode_plus(sample['prompt'], return_tensors='pt')\n",
    "\n",
    "summary = tokenizer.batch_decode(\n",
    "    finetuned_qa_model.generate(**_in, \n",
    "                                num_beams=3,\n",
    "                                top_p=0.5,\n",
    "                                top_k=3,\n",
    "                                do_sample=True, \n",
    "                                early_stopping=True,\n",
    "                                num_return_sequences=5\n",
    "                               ), \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "print(\"fine-tuned flan-t5 answer:\\n\" + '\\n'.join(summary))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "31a2c7d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9001, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_in['labels'] = tokenizer.encode(sample['answer'], return_tensors='pt')\n",
    "\n",
    "response = finetuned_qa_model(**_in)\n",
    "\n",
    "response.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "a7d4ac2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monica is to buy a day spa gift card for but Monica isn't interested in buying.\n"
     ]
    }
   ],
   "source": [
    "call_response = tokenizer.batch_decode(response.logits.argmax(2), skip_special_tokens=True)[0]\n",
    "print(call_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "8e8ebc76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /Users/sinanozdemir/.cache/torch/sentence_transformers/sentence-transformers_msmarco-distilbert-base-v4/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"/Users/sinanozdemir/.cache/torch/sentence_transformers/sentence-transformers_msmarco-distilbert-base-v4/\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file /Users/sinanozdemir/.cache/torch/sentence_transformers/sentence-transformers_msmarco-distilbert-base-v4/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DistilBertModel.\n",
      "\n",
      "All the weights of DistilBertModel were initialized from the model checkpoint at /Users/sinanozdemir/.cache/torch/sentence_transformers/sentence-transformers_msmarco-distilbert-base-v4/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n",
      "loading configuration file /Users/sinanozdemir/.cache/torch/sentence_transformers/sentence-transformers_msmarco-distilbert-base-v4/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"/Users/sinanozdemir/.cache/torch/sentence_transformers/sentence-transformers_msmarco-distilbert-base-v4/\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file /Users/sinanozdemir/.cache/torch/sentence_transformers/sentence-transformers_msmarco-distilbert-base-v4/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"/Users/sinanozdemir/.cache/torch/sentence_transformers/sentence-transformers_msmarco-distilbert-base-v4/\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "nli_model = SentenceTransformer('msmarco-distilbert-base-v4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "a23cd45b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sophia: What should  \t\t Monica isn't interested in buying a day spa gift c \t\t Score: 0.5544\n",
      "Sophia: What should  \t\t Monica isn't interested in buying a day spa gift c \t\t Score: 0.5544\n",
      "Sophia: What should  \t\t Monica isn't interested in buying a day spa gift c \t\t Score: 0.5544\n",
      "Sophia: What should  \t\t Monica isn't interested in buying a day spa gift c \t\t Score: 0.5544\n",
      "Sophia: What should  \t\t Monica isn't interested in buying a day spa gift c \t\t Score: 0.5544\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Single list of sentences\n",
    "sentences = [sample['prompt'].replace(f'{question}: ', '')] + summary\n",
    "\n",
    "#Compute embeddings\n",
    "embeddings = nli_model.encode(sentences, convert_to_tensor=True)\n",
    "\n",
    "#Compute cosine-similarities for each sentence with each other sentence\n",
    "cosine_scores = util.cos_sim(embeddings, embeddings)\n",
    "\n",
    "#Find the pairs with the highest cosine similarity scores\n",
    "pairs = []\n",
    "for i in range(1):\n",
    "    for j in range(i+1, len(cosine_scores)):\n",
    "        pairs.append({'index': [i, j], 'score': cosine_scores[i][j]})\n",
    "\n",
    "#Sort scores in decreasing order\n",
    "pairs = sorted(pairs, key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "for pair in pairs[0:10]:\n",
    "    i, j = pair['index']\n",
    "    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences[i][:20], sentences[j][:50], pair['score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace7cf8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f484cbd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "f5fd14d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "from random import seed, shuffle\n",
    "from sentence_transformers import InputExample, losses, evaluation\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "6d3a28fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': \"Who did Richard take Hannah's parking spot from?\",\n",
       " 'prompt': \"Who did Richard take Hannah's parking spot from?: Mariana: Hi, just a quick question. Do you know if the readings for the next session of Stephenâ€™s seminar are in the print shop?\\r\\nRita: No idea, sorry\\r\\nChae-yeong: The only thing I know is that they were not yet there on Monday\\r\\nArthur: Yer, Iâ€™ve made the mistake of going on Monday as well and I can confirm the texts were not there and the staff was as rude as always\\r\\nMariana: Sounds familiar\\r\\nRita: Iâ€™ll go tomorrow morning and let you know if the texts are available\\r\\nRita: I canâ€™t buy the copies for you because they never have enough at hand but Iâ€™ll just ask them to print more for later\\r\\nChae-yeong: No worries\\r\\nArthur: Yeah, just let us know if theyâ€™re ready, thatâ€™s a huge favour already\\r\\nRita: Cool. Will do! xx\",\n",
       " 'answer': 'I cannot be sure given this transcript'}"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "87f304c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y9/9xqbqkg90tnc0cmm0dxt985m0000gn/T/ipykernel_32392/2355648357.py:5: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  answer = record['answer'] if 'I cannot' not in record['answer'] else conv_df[conv_df['prompt'].str.contains(record[\"question\"])][~conv_df['answer'].str.contains('I cannot')].iloc[0].answer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'guid': '',\n",
       " 'texts': ('Tomorrow he will send George the tracking number of the goods he has shipped to him',\n",
       "  'Mark: I just shipped the goods\\r\\nMark: Tomorrow Iâ€™ll send you the tracking number\\r\\nGeorge: Thanks!'),\n",
       " 'label': 1.0}"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_examples = []\n",
    "\n",
    "for i, record in enumerate(conv_qa_dataset['train']):\n",
    "    try:\n",
    "        answer = record['answer'] if 'I cannot' not in record['answer'] else conv_df[conv_df['prompt'].str.contains(record[\"question\"])][~conv_df['answer'].str.contains('I cannot')].iloc[0].answer\n",
    "    except:\n",
    "        continue\n",
    "    if 'I cannot' in answer:\n",
    "        continue\n",
    "    train_examples.append(\n",
    "        InputExample(\n",
    "            texts=(\n",
    "                answer, \n",
    "                record['prompt'].replace(record['question']+': ', '')\n",
    "            ), \n",
    "            label=float('I cannot be sure' not in record['answer'])\n",
    "    )\n",
    "    )\n",
    "train_examples[0].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "678fa2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y9/9xqbqkg90tnc0cmm0dxt985m0000gn/T/ipykernel_32392/1361839398.py:5: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  answer = record['answer'] if 'I cannot' not in record['answer'] else conv_df[conv_df['prompt'].str.contains(record[\"question\"])][~conv_df['answer'].str.contains('I cannot')].iloc[0].answer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'guid': '',\n",
       " 'texts': (\"Lawrence and Julius are offended by Manutd's performance this season\",\n",
       "  'Julius: dude, your assessment of manutd\\r\\nLawrence: i have nothing to say, im so offended and hopeless of them this season\\r\\nJulius: me too\\r\\nLawrence: i dont even know whats wrong with the team\\r\\nJulius: the quality is there but nothing is happening\\r\\nLawrence: the players look tired of something\\r\\nJulius:  with mourinhos conservative football!!\\r\\nLawrence: its so boring\\r\\nJulius: so lifeless\\r\\nLawrence: man!!\\r\\nJulius: it needs to change, hope the board sees it\\r\\nLawrence: sooner than later\\r\\nJulius: yeah\\r\\nLawrence: yeah'),\n",
       " 'label': 1.0}"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_examples = []\n",
    "\n",
    "for i, record in enumerate(conv_qa_dataset['test']):\n",
    "    try:\n",
    "        answer = record['answer'] if 'I cannot' not in record['answer'] else conv_df[conv_df['prompt'].str.contains(record[\"question\"])][~conv_df['answer'].str.contains('I cannot')].iloc[0].answer\n",
    "    except:\n",
    "        continue\n",
    "    if 'I cannot' in answer:\n",
    "        continue\n",
    "    test_examples.append(\n",
    "        InputExample(\n",
    "            texts=(\n",
    "                answer, \n",
    "                record['prompt'].replace(record['question']+': ', '')\n",
    "            ), \n",
    "            label=float('I cannot be sure' not in record['answer'])\n",
    "    )\n",
    "    )\n",
    "test_examples[0].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "e94e8e20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "408"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "791646f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /Users/sinanozdemir/.cache/torch/sentence_transformers/sentence-transformers_msmarco-distilbert-base-v4/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"/Users/sinanozdemir/.cache/torch/sentence_transformers/sentence-transformers_msmarco-distilbert-base-v4/\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file /Users/sinanozdemir/.cache/torch/sentence_transformers/sentence-transformers_msmarco-distilbert-base-v4/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DistilBertModel.\n",
      "\n",
      "All the weights of DistilBertModel were initialized from the model checkpoint at /Users/sinanozdemir/.cache/torch/sentence_transformers/sentence-transformers_msmarco-distilbert-base-v4/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n",
      "loading configuration file /Users/sinanozdemir/.cache/torch/sentence_transformers/sentence-transformers_msmarco-distilbert-base-v4/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"/Users/sinanozdemir/.cache/torch/sentence_transformers/sentence-transformers_msmarco-distilbert-base-v4/\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file /Users/sinanozdemir/.cache/torch/sentence_transformers/sentence-transformers_msmarco-distilbert-base-v4/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"/Users/sinanozdemir/.cache/torch/sentence_transformers/sentence-transformers_msmarco-distilbert-base-v4/\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8189468014796153"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "nli_model = SentenceTransformer('msmarco-distilbert-base-v4')\n",
    "\n",
    "\n",
    "# Evaluation data, sentences1 and sentences2 are lists of questions and context respectively and scores are 0 or 1\n",
    "sentences1, sentences2, scores = [_.texts[0] for _ in train_examples], [_.texts[1] for _ in train_examples], [_.label for _ in train_examples]\n",
    "\n",
    "# evaluator will evaluate embedding closeness\n",
    "evaluator = evaluation.EmbeddingSimilarityEvaluator(sentences1, sentences2, scores)\n",
    "\n",
    "nli_model.evaluate(evaluator)  # initial evalaution (higher embedding similarity is better)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "4661ce63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6646b02d4ae048b9a8f59be22b19416f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e13dea8854de4c53973420628a9f4f64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in conv_answer_similarity/results/config.json\n",
      "Model weights saved in conv_answer_similarity/results/pytorch_model.bin\n",
      "tokenizer config file saved in conv_answer_similarity/results/tokenizer_config.json\n",
      "Special tokens file saved in conv_answer_similarity/results/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "573ff0369cfa47b1b77984918c8c1a3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in conv_answer_similarity/results/config.json\n",
      "Model weights saved in conv_answer_similarity/results/pytorch_model.bin\n",
      "tokenizer config file saved in conv_answer_similarity/results/tokenizer_config.json\n",
      "Special tokens file saved in conv_answer_similarity/results/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4596dc32815749c793693503a68dfab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in conv_answer_similarity/results/config.json\n",
      "Model weights saved in conv_answer_similarity/results/pytorch_model.bin\n",
      "tokenizer config file saved in conv_answer_similarity/results/tokenizer_config.json\n",
      "Special tokens file saved in conv_answer_similarity/results/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeb37398f1fd465f9592e008186bafa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in conv_answer_similarity/results/config.json\n",
      "Model weights saved in conv_answer_similarity/results/pytorch_model.bin\n",
      "tokenizer config file saved in conv_answer_similarity/results/tokenizer_config.json\n",
      "Special tokens file saved in conv_answer_similarity/results/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6577671556243b49198b23123b68f2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in conv_answer_similarity/results/config.json\n",
      "Model weights saved in conv_answer_similarity/results/pytorch_model.bin\n",
      "tokenizer config file saved in conv_answer_similarity/results/tokenizer_config.json\n",
      "Special tokens file saved in conv_answer_similarity/results/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "352435e2a75546ec8847f15fa80b162f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in conv_answer_similarity/results/config.json\n",
      "Model weights saved in conv_answer_similarity/results/pytorch_model.bin\n",
      "tokenizer config file saved in conv_answer_similarity/results/tokenizer_config.json\n",
      "Special tokens file saved in conv_answer_similarity/results/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a68a0db60e75406c9b51dbd65def3a7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in conv_answer_similarity/results/config.json\n",
      "Model weights saved in conv_answer_similarity/results/pytorch_model.bin\n",
      "tokenizer config file saved in conv_answer_similarity/results/tokenizer_config.json\n",
      "Special tokens file saved in conv_answer_similarity/results/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a91dd9fa6f747f6b7ce4fc328cacbd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in conv_answer_similarity/results/config.json\n",
      "Model weights saved in conv_answer_similarity/results/pytorch_model.bin\n",
      "tokenizer config file saved in conv_answer_similarity/results/tokenizer_config.json\n",
      "Special tokens file saved in conv_answer_similarity/results/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "655ff78e72db423784a5ffd0681b1767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in conv_answer_similarity/results/config.json\n",
      "Model weights saved in conv_answer_similarity/results/pytorch_model.bin\n",
      "tokenizer config file saved in conv_answer_similarity/results/tokenizer_config.json\n",
      "Special tokens file saved in conv_answer_similarity/results/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6badcf5011d34a0981d739c98bdb7ae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in conv_answer_similarity/results/config.json\n",
      "Model weights saved in conv_answer_similarity/results/pytorch_model.bin\n",
      "tokenizer config file saved in conv_answer_similarity/results/tokenizer_config.json\n",
      "Special tokens file saved in conv_answer_similarity/results/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "# Define the train dataset, a dataloader and the train loss\n",
    "# A data loader is the object that specifically shuffles/grabs batches of data from a Dataset\n",
    "# We don't usually have to explicitly create one using the Trainer because it has a default loader built in\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=32)\n",
    "\n",
    "train_loss = losses.CosineSimilarityLoss(nli_model)\n",
    "\n",
    "# Fine-tune the model using the fit method\n",
    "nli_model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)], \n",
    "    output_path='conv_answer_similarity/results',\n",
    "    epochs=10,\n",
    "    evaluator=evaluator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "aa5da316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8284796036724091"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nli_model.evaluate(evaluator)  # second evalaution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "c7543b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file conv_answer_similarity/results/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"conv_answer_similarity/results/\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file conv_answer_similarity/results/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DistilBertModel.\n",
      "\n",
      "All the weights of DistilBertModel were initialized from the model checkpoint at conv_answer_similarity/results/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sophia: What should  \t\t Monica isn't interested in buying a day spa gift c \t\t Score: 0.6484\n",
      "Sophia: What should  \t\t Monica isn't interested in buying a day spa gift c \t\t Score: 0.6484\n",
      "Sophia: What should  \t\t Monica isn't interested in buying a day spa gift c \t\t Score: 0.6484\n",
      "Sophia: What should  \t\t Monica isn't interested in buying a day spa gift c \t\t Score: 0.6484\n",
      "Sophia: What should  \t\t Monica isn't interested in buying a day spa gift c \t\t Score: 0.6484\n"
     ]
    }
   ],
   "source": [
    "# load fine-tuned IR model\n",
    "finetuned_bi_encoder = SentenceTransformer('conv_answer_similarity/results')\n",
    "\n",
    "#Compute embeddings\n",
    "embeddings = finetuned_bi_encoder.encode(sentences, convert_to_tensor=True)\n",
    "\n",
    "#Compute cosine-similarities for each sentence with each other sentence\n",
    "cosine_scores = util.cos_sim(embeddings, embeddings)\n",
    "\n",
    "#Find the pairs with the highest cosine similarity scores\n",
    "pairs = []\n",
    "for i in range(1):\n",
    "    for j in range(i+1, len(cosine_scores)):\n",
    "        pairs.append({'index': [i, j], 'score': cosine_scores[i][j]})\n",
    "\n",
    "#Sort scores in decreasing order\n",
    "pairs = sorted(pairs, key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "for pair in pairs[0:10]:\n",
    "    i, j = pair['index']\n",
    "    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences[i][:20], sentences[j][:50], pair['score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce33c8fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
